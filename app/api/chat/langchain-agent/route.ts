/**
 * Endpoint Unificado del Agente Legal con LangChain
 * 
 * Este endpoint usa LangChain para implementar un agente con tool calling nativo.
 * 
 * CaracterÃ­sticas:
 * - Soporta mÃºltiples modelos (Kimi K2, Tongyi, GPT-4o, Claude)
 * - Tool calling nativo (el modelo decide cuÃ¡ndo usar herramientas)
 * - Streaming REAL de respuestas y razonamiento
 * - Manejo de historial de conversaciÃ³n
 * 
 * Modelos recomendados:
 * - moonshotai/kimi-k2-thinking: Razonamiento profundo (M1 Pro)
 * - alibaba/tongyi-deepresearch-30b-a3b: InvestigaciÃ³n profunda (M1)
 * 
 * Formato de streaming (JSON Lines):
 * - {"type": "thinking", "content": "..."} - Proceso de razonamiento
 * - {"type": "tool_start", "tool": "...", "input": "..."} - Inicio de herramienta
 * - {"type": "tool_end", "tool": "...", "output": "..."} - Fin de herramienta  
 * - {"type": "token", "content": "..."} - Token de respuesta
 * - {"type": "sources", "sources": [...]} - Fuentes encontradas
 * - {"type": "done"} - Fin del stream
 */

import { NextRequest, NextResponse } from "next/server"
import { HumanMessage, AIMessage, BaseMessage } from "@langchain/core/messages"
import { LegalAgent, getModelConfig, RESEARCH_MODELS } from "@/lib/langchain"
import { BaseCallbackHandler } from "@langchain/core/callbacks/base"

export const runtime = "nodejs"
export const maxDuration = 180 // 3 minutos para investigaciÃ³n completa

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// INTERFACES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

interface RequestBody {
  chatSettings: {
    model: string
    temperature?: number
  }
  messages: Array<{
    role: "system" | "user" | "assistant"
    content: string
  }>
  chatId?: string
  userId?: string
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CACHE DE AGENTES (por sesiÃ³n)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Cache simple de agentes por chatId para reutilizar en conversaciones
const agentCache = new Map<string, { agent: LegalAgent; lastUsed: Date }>()

// Limpiar agentes inactivos cada 10 minutos
const CACHE_TTL = 10 * 60 * 1000 // 10 minutos

function cleanupCache() {
  const now = Date.now()
  for (const [key, value] of agentCache.entries()) {
    if (now - value.lastUsed.getTime() > CACHE_TTL) {
      agentCache.delete(key)
    }
  }
}

// Ejecutar cleanup periÃ³dicamente (solo en el primer request)
let cleanupInterval: NodeJS.Timeout | null = null

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STREAMING CALLBACK HANDLER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Callback handler que emite eventos de streaming
 */
class StreamingCallbackHandler extends BaseCallbackHandler {
  name = "streaming_handler"
  private encoder: TextEncoder
  private controller: ReadableStreamDefaultController<Uint8Array>

  constructor(controller: ReadableStreamDefaultController<Uint8Array>) {
    super()
    this.encoder = new TextEncoder()
    this.controller = controller
  }

  private emit(event: object) {
    try {
      const data = JSON.stringify(event) + '\n'
      this.controller.enqueue(this.encoder.encode(data))
    } catch (e) {
      console.error('Error emitting event:', e)
    }
  }

  // Cuando el LLM empieza a generar
  async handleLLMStart(llm: any, prompts: string[]) {
    this.emit({ type: 'thinking', content: 'ğŸ§  Analizando la consulta...' })
  }

  // Cuando recibimos tokens del LLM (razonamiento/respuesta)
  async handleLLMNewToken(token: string) {
    // Detectar si es parte del razonamiento (thinking) o respuesta final
    // Los modelos thinking suelen incluir tags especiales
    if (token.includes('<think>') || token.includes('</think>')) {
      // No emitir los tags, solo el contenido
      return
    }
    
    this.emit({ type: 'token', content: token })
  }

  // Cuando el LLM termina
  async handleLLMEnd(output: any) {
    // Si hay reasoning/thinking en el output, emitirlo
    const reasoning = output?.generations?.[0]?.[0]?.message?.additional_kwargs?.reasoning
    if (reasoning) {
      this.emit({ type: 'thinking', content: reasoning })
    }
  }

  // Cuando se inicia una herramienta
  async handleToolStart(tool: any, input: string) {
    const toolName = tool?.name || 'herramienta'
    this.emit({ 
      type: 'tool_start', 
      tool: toolName, 
      input: input.substring(0, 100) + (input.length > 100 ? '...' : '')
    })
  }

  // Cuando termina una herramienta
  async handleToolEnd(output: string) {
    // Resumir el output si es muy largo
    const summary = output.length > 200 
      ? output.substring(0, 200) + '... (ver fuentes abajo)'
      : output
    this.emit({ type: 'tool_end', output: summary })
  }

  // Cuando hay un error en una herramienta
  async handleToolError(err: Error) {
    this.emit({ type: 'tool_error', error: err.message })
  }

  // Cuando el agente toma una acciÃ³n
  async handleAgentAction(action: any) {
    this.emit({ 
      type: 'thinking', 
      content: `ğŸ“‹ DecidÃ­ usar: ${action.tool} para "${action.toolInput?.query || action.toolInput?.url || '...'}"` 
    })
  }

  // Cuando el agente termina
  async handleAgentEnd(output: any) {
    this.emit({ type: 'agent_done' })
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FUNCIONES AUXILIARES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Convierte mensajes del formato del chat al formato de LangChain
 */
function convertMessages(messages: RequestBody['messages']): BaseMessage[] {
  return messages
    .filter(m => m.role !== 'system') // El system prompt lo maneja el agente
    .map(msg => {
      if (msg.role === 'user') {
        return new HumanMessage(msg.content)
      } else {
        return new AIMessage(msg.content)
      }
    })
}

/**
 * Obtiene o crea un agente para un chat especÃ­fico
 */
async function getOrCreateAgent(
  chatId: string, 
  modelId: string, 
  temperature: number
): Promise<LegalAgent> {
  const cacheKey = `${chatId}-${modelId}`
  
  const cached = agentCache.get(cacheKey)
  if (cached) {
    cached.lastUsed = new Date()
    console.log(`â™»ï¸ Reutilizando agente en cachÃ©: ${cacheKey}`)
    return cached.agent
  }

  console.log(`ğŸ†• Creando nuevo agente: ${cacheKey}`)
  const agent = await LegalAgent.create({
    modelId,
    temperature,
    maxIterations: 6,
    verbose: process.env.NODE_ENV === 'development'
  })

  agentCache.set(cacheKey, { agent, lastUsed: new Date() })
  return agent
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HANDLER PRINCIPAL
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export async function POST(request: NextRequest) {
  const startTime = Date.now()
  
  // Iniciar cleanup si no estÃ¡ corriendo
  if (!cleanupInterval) {
    cleanupInterval = setInterval(cleanupCache, CACHE_TTL)
  }

  console.log(`\n${'â•'.repeat(80)}`)
  console.log(`ğŸ¤– LANGCHAIN AGENT - ENDPOINT UNIFICADO`)
  console.log(`${'â•'.repeat(80)}`)

  try {
    const body = await request.json() as RequestBody
    const { chatSettings, messages, chatId, userId } = body

    // Validar API Key
    const apiKey = process.env.OPENROUTER_API_KEY
    if (!apiKey) {
      return NextResponse.json(
        { error: "OPENROUTER_API_KEY no configurada" },
        { status: 500 }
      )
    }

    // Determinar modelo a usar
    const modelId = chatSettings.model || 'alibaba/tongyi-deepresearch-30b-a3b'
    const temperature = chatSettings.temperature ?? 0.3

    // Verificar que el modelo soporte tools
    const modelConfig = getModelConfig(modelId)
    if (modelConfig && !modelConfig.supportsTools) {
      console.warn(`âš ï¸ Modelo ${modelId} no soporta tools, usando fallback`)
      // PodrÃ­as hacer fallback a otro modelo aquÃ­
    }

    // Extraer el Ãºltimo mensaje del usuario
    const userMessages = messages.filter(m => m.role === 'user')
    const lastUserMessage = userMessages[userMessages.length - 1]?.content || ''

    console.log(`ğŸ“ Query: "${lastUserMessage.substring(0, 100)}..."`)
    console.log(`ğŸ¤– Modelo: ${modelId}`)
    console.log(`ğŸŒ¡ï¸ Temperature: ${temperature}`)
    console.log(`ğŸ’¬ Chat ID: ${chatId || 'N/A'}`)
    console.log(`ğŸ‘¤ User ID: ${userId || 'N/A'}`)

    // Obtener o crear agente
    const effectiveChatId = chatId || `temp-${Date.now()}`
    const agent = await getOrCreateAgent(effectiveChatId, modelId, temperature)

    // Convertir historial (excluyendo el Ãºltimo mensaje del usuario)
    const chatHistory = convertMessages(messages.slice(0, -1))

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // EJECUTAR AGENTE CON STREAMING REAL
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.log(`ğŸš€ Ejecutando agente con streaming real...`)

    // Crear stream de respuesta con eventos JSON
    const encoder = new TextEncoder()
    
    const stream = new ReadableStream({
      async start(controller) {
        // Helper para emitir eventos
        const emit = (event: object) => {
          try {
            controller.enqueue(encoder.encode(JSON.stringify(event) + '\n'))
          } catch (e) {
            console.error('Error emitting:', e)
          }
        }

        try {
          // Emitir evento de inicio
          emit({ type: 'thinking', content: 'ğŸ§  Analizando tu consulta legal...' })

          // Ejecutar el agente
          const result = await agent.invoke({
            input: lastUserMessage,
            chatHistory
          })

          // Emitir informaciÃ³n sobre herramientas usadas
          if (result.toolsUsed && result.toolsUsed.length > 0) {
            emit({ 
              type: 'thinking', 
              content: `ğŸ”§ Herramientas utilizadas: ${result.toolsUsed.join(', ')}` 
            })
          }

          // Emitir pasos intermedios como razonamiento
          if (result.intermediateSteps && result.intermediateSteps.length > 0) {
            for (const step of result.intermediateSteps) {
              if (step.action?.tool) {
                emit({ 
                  type: 'tool_start', 
                  tool: step.action.tool,
                  input: typeof step.action.toolInput === 'string' 
                    ? step.action.toolInput.substring(0, 100)
                    : JSON.stringify(step.action.toolInput).substring(0, 100)
                })
              }
              if (step.observation) {
                const obsPreview = typeof step.observation === 'string'
                  ? step.observation.substring(0, 150)
                  : 'Resultados obtenidos'
                emit({ type: 'tool_end', output: obsPreview + '...' })
              }
            }
          }

          // Emitir fin del razonamiento
          emit({ type: 'thinking_done' })

          // Limpiar la respuesta del modelo
          let cleanOutput = result.output
          
          // Limpieza de formato
          cleanOutput = cleanOutput
            .replace(/\*{0,2}Fuentes consultadas\*{0,2}\s*\n+/gi, '')
            .replace(/\d+\s*referencias?\s*\n+/gi, '')
            .replace(/\n+---\n*\*{0,2}Fuentes?\s*(consultadas|legales?)?\*{0,2}:?\s*\n*$/gi, '')
            .replace(/\n+\*{0,2}Fuentes?\s*(consultadas|legales?)?\*{0,2}:?\s*\n*$/gi, '')
            .replace(/\n*\*{0,2}(Advertencia|Nota importante|Importante|Disclaimer):?\*{0,2}[^]*?(consultar?|abogado|profesional|asesor)[^]*?\.?\n*/gi, '\n')
            .replace(/\n{3,}/g, '\n\n')
            .trim()
          
          // Emitir respuesta token por token (streaming real)
          const words = cleanOutput.split(' ')
          
          for (let i = 0; i < words.length; i++) {
            const word = words[i] + (i < words.length - 1 ? ' ' : '')
            emit({ type: 'token', content: word })
            
            // PequeÃ±a pausa para efecto de streaming visual
            await new Promise(resolve => setTimeout(resolve, 15))
          }

          // Emitir fuentes si existen
          if (result.sources && result.sources.length > 0) {
            const validSources = result.sources.filter(s => 
              s.url && s.url.startsWith('http') && s.url.length > 10
            )
            
            const uniqueSources = validSources.filter((s, i, arr) => 
              arr.findIndex(x => x.url === s.url) === i
            )
            
            if (uniqueSources.length > 0) {
              emit({ type: 'sources', sources: uniqueSources })
              
              // TambiÃ©n emitir como texto para compatibilidad
              const sourcesSection = `\n\n---\n\nğŸ“š **Fuentes consultadas:**\n\n${
                uniqueSources.map((s, i) => {
                  let title = s.title || 'Fuente legal'
                  try {
                    const url = new URL(s.url)
                    const hostname = url.hostname.replace('www.', '')
                    const knownDomains: Record<string, string> = {
                      'secretariasenado.gov.co': 'SecretarÃ­a del Senado',
                      'corteconstitucional.gov.co': 'Corte Constitucional',
                      'consejodeestado.gov.co': 'Consejo de Estado',
                      'suin-juriscol.gov.co': 'SUIN-Juriscol',
                    }
                    if (!title || title === s.url || title.length < 3) {
                      title = knownDomains[hostname] || hostname
                    }
                  } catch {}
                  return `${i + 1}. [${title}](${s.url})`
                }).join('\n')
              }`
              emit({ type: 'token', content: sourcesSection })
            }
          }

          // Emitir evento de finalizaciÃ³n
          const processingTime = ((Date.now() - startTime) / 1000).toFixed(1)
          emit({ 
            type: 'done',
            metadata: {
              model: modelId,
              processingTime: processingTime + 's',
              toolsUsed: result.toolsUsed || [],
              sourcesCount: result.sources?.length || 0
            }
          })
          
          controller.close()

          console.log(`\n${'â•'.repeat(60)}`)
          console.log(`âœ… RESPUESTA COMPLETADA (Streaming real)`)
          console.log(`   â±ï¸ Tiempo: ${processingTime}s`)
          console.log(`   ğŸ”§ Tools: ${result.toolsUsed?.join(', ') || 'Ninguna'}`)
          console.log(`   ğŸ“š Fuentes: ${result.sources?.length || 0}`)
          console.log(`${'â•'.repeat(60)}\n`)

        } catch (error) {
          console.error('âŒ Error en streaming:', error)
          emit({ type: 'error', message: 'Hubo un error procesando tu consulta. Por favor, intenta de nuevo.' })
          controller.close()
        }
      }
    })
    
    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream; charset=utf-8',
        'Transfer-Encoding': 'chunked',
        'X-Model-Used': modelId,
        'X-Streaming': 'true',
        'Cache-Control': 'no-cache'
      }
    })

  } catch (error: any) {
    console.error(`âŒ Error en LangChain Agent:`, error)
    
    return NextResponse.json(
      { 
        error: error.message || "Error procesando la consulta",
        details: process.env.NODE_ENV === 'development' ? error.toString() : undefined
      },
      { status: 500 }
    )
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ENDPOINT GET - INFORMACIÃ“N DEL SERVICIO
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export async function GET() {
  const hasOpenRouter = Boolean(process.env.OPENROUTER_API_KEY)
  const hasSerper = Boolean(process.env.SERPER_API_KEY)

  return NextResponse.json({
    status: "ok",
    endpoint: "LangChain Agent - Unified Legal Assistant",
    version: "1.0.0",
    features: [
      "Tool calling nativo",
      "MÃºltiples modelos soportados (Kimi K2, Tongyi, GPT-4o, Claude)",
      "El modelo decide autÃ³nomamente cuÃ¡ndo usar herramientas",
      "Streaming de respuestas",
      "Cache de agentes por sesiÃ³n"
    ],
    recommendedModels: RESEARCH_MODELS,
    tools: [
      "search_legal_official",
      "search_legal_academic", 
      "search_general_web",
      "extract_web_content",
      "verify_sources"
    ],
    apiKeys: {
      openrouter: hasOpenRouter ? "âœ… Configurada" : "âŒ Falta",
      serper: hasSerper ? "âœ… Configurada" : "âŒ Falta"
    },
    cacheStats: {
      activeAgents: agentCache.size
    }
  })
}



